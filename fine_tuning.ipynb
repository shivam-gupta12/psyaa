{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BlenderbotTokenizer, BlenderbotForCausalLM, BlenderbotConfig\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/blenderbot-400M-distill were not used when initializing BlenderbotForCausalLM: ['model.encoder.layers.0.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layer_norm.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.1.fc1.bias', 'model.shared.weight', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.encoder.embed_positions.weight', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.encoder.layer_norm.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.embed_tokens.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'final_logits_bias']\n",
      "- This IS expected if you are initializing BlenderbotForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BlenderbotForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BlenderbotTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "\n",
    "model_config = BlenderbotConfig.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "\n",
    "model = BlenderbotForCausalLM.from_pretrained(\"facebook/blenderbot-400M-distill\", config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pb/3pqt97t1701c0435w57nfgrh0000gn/T/ipykernel_30597/3380662777.py:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df = pd.read_csv(\"/Users/damodargupta/Desktop/sentence-emotion analysis model- psyaa/fine-tuning-blenderbot/train.csv\",  error_bad_lines=False)\n",
      "Skipping line 2355: expected 8 fields, saw 10\n",
      "Skipping line 36628: expected 8 fields, saw 12\n",
      "Skipping line 49433: expected 8 fields, saw 10\n",
      "Skipping line 56957: expected 8 fields, saw 10\n",
      "Skipping line 65019: expected 8 fields, saw 10\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv_id</th>\n",
       "      <th>utterance_idx</th>\n",
       "      <th>context</th>\n",
       "      <th>prompt</th>\n",
       "      <th>speaker_idx</th>\n",
       "      <th>utterance</th>\n",
       "      <th>selfeval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hit:0_conv:1</th>\n",
       "      <td>1</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>1</td>\n",
       "      <td>I remember going to see the fireworks with my ...</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit:0_conv:1</th>\n",
       "      <td>2</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>0</td>\n",
       "      <td>Was this a friend you were in love with_comma_...</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit:0_conv:1</th>\n",
       "      <td>3</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>1</td>\n",
       "      <td>This was a best friend. I miss her.</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit:0_conv:1</th>\n",
       "      <td>4</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>0</td>\n",
       "      <td>Where has she gone?</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit:0_conv:1</th>\n",
       "      <td>5</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>1</td>\n",
       "      <td>We no longer talk.</td>\n",
       "      <td>5|5|5_2|2|5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit:12424_conv:24848</th>\n",
       "      <td>5</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>I found some pictures of my grandma in the att...</td>\n",
       "      <td>389</td>\n",
       "      <td>Yeah reminds me of the good old days.  I miss ...</td>\n",
       "      <td>5|5|5_5|5|5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit:12424_conv:24849</th>\n",
       "      <td>1</td>\n",
       "      <td>surprised</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>294</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>5|5|5_5|5|5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit:12424_conv:24849</th>\n",
       "      <td>2</td>\n",
       "      <td>surprised</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>389</td>\n",
       "      <td>Oh hey that's awesome!  That is awesome right?</td>\n",
       "      <td>5|5|5_5|5|5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit:12424_conv:24849</th>\n",
       "      <td>3</td>\n",
       "      <td>surprised</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>294</td>\n",
       "      <td>It is soooo awesome.  We have been wanting a b...</td>\n",
       "      <td>5|5|5_5|5|5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit:12424_conv:24849</th>\n",
       "      <td>4</td>\n",
       "      <td>surprised</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>389</td>\n",
       "      <td>That is awesome!!!! Congratulations!</td>\n",
       "      <td>5|5|5_5|5|5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76668 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      conv_id utterance_idx  \\\n",
       "hit:0_conv:1                1   sentimental   \n",
       "hit:0_conv:1                2   sentimental   \n",
       "hit:0_conv:1                3   sentimental   \n",
       "hit:0_conv:1                4   sentimental   \n",
       "hit:0_conv:1                5   sentimental   \n",
       "...                       ...           ...   \n",
       "hit:12424_conv:24848        5   sentimental   \n",
       "hit:12424_conv:24849        1     surprised   \n",
       "hit:12424_conv:24849        2     surprised   \n",
       "hit:12424_conv:24849        3     surprised   \n",
       "hit:12424_conv:24849        4     surprised   \n",
       "\n",
       "                                                                context  \\\n",
       "hit:0_conv:1          I remember going to the fireworks with my best...   \n",
       "hit:0_conv:1          I remember going to the fireworks with my best...   \n",
       "hit:0_conv:1          I remember going to the fireworks with my best...   \n",
       "hit:0_conv:1          I remember going to the fireworks with my best...   \n",
       "hit:0_conv:1          I remember going to the fireworks with my best...   \n",
       "...                                                                 ...   \n",
       "hit:12424_conv:24848  I found some pictures of my grandma in the att...   \n",
       "hit:12424_conv:24849  I woke up this morning to my wife telling me s...   \n",
       "hit:12424_conv:24849  I woke up this morning to my wife telling me s...   \n",
       "hit:12424_conv:24849  I woke up this morning to my wife telling me s...   \n",
       "hit:12424_conv:24849  I woke up this morning to my wife telling me s...   \n",
       "\n",
       "                      prompt  \\\n",
       "hit:0_conv:1               1   \n",
       "hit:0_conv:1               0   \n",
       "hit:0_conv:1               1   \n",
       "hit:0_conv:1               0   \n",
       "hit:0_conv:1               1   \n",
       "...                      ...   \n",
       "hit:12424_conv:24848     389   \n",
       "hit:12424_conv:24849     294   \n",
       "hit:12424_conv:24849     389   \n",
       "hit:12424_conv:24849     294   \n",
       "hit:12424_conv:24849     389   \n",
       "\n",
       "                                                            speaker_idx  \\\n",
       "hit:0_conv:1          I remember going to see the fireworks with my ...   \n",
       "hit:0_conv:1          Was this a friend you were in love with_comma_...   \n",
       "hit:0_conv:1                        This was a best friend. I miss her.   \n",
       "hit:0_conv:1                                        Where has she gone?   \n",
       "hit:0_conv:1                                         We no longer talk.   \n",
       "...                                                                 ...   \n",
       "hit:12424_conv:24848  Yeah reminds me of the good old days.  I miss ...   \n",
       "hit:12424_conv:24849  I woke up this morning to my wife telling me s...   \n",
       "hit:12424_conv:24849     Oh hey that's awesome!  That is awesome right?   \n",
       "hit:12424_conv:24849  It is soooo awesome.  We have been wanting a b...   \n",
       "hit:12424_conv:24849               That is awesome!!!! Congratulations!   \n",
       "\n",
       "                        utterance selfeval  \n",
       "hit:0_conv:1          5|5|5_2|2|5      NaN  \n",
       "hit:0_conv:1          5|5|5_2|2|5      NaN  \n",
       "hit:0_conv:1          5|5|5_2|2|5      NaN  \n",
       "hit:0_conv:1          5|5|5_2|2|5      NaN  \n",
       "hit:0_conv:1          5|5|5_2|2|5      NaN  \n",
       "...                           ...      ...  \n",
       "hit:12424_conv:24848  5|5|5_5|5|5      NaN  \n",
       "hit:12424_conv:24849  5|5|5_5|5|5      NaN  \n",
       "hit:12424_conv:24849  5|5|5_5|5|5      NaN  \n",
       "hit:12424_conv:24849  5|5|5_5|5|5      NaN  \n",
       "hit:12424_conv:24849  5|5|5_5|5|5      NaN  \n",
       "\n",
       "[76668 rows x 7 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/damodargupta/Desktop/sentence-emotion analysis model- psyaa/fine-tuning-blenderbot/train.csv\",  error_bad_lines=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop('selfeval',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utterance_idx</th>\n",
       "      <th>context</th>\n",
       "      <th>speaker_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hit:0_conv:1</th>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>I remember going to see the fireworks with my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit:0_conv:1</th>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>Was this a friend you were in love with_comma_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit:0_conv:1</th>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>This was a best friend. I miss her.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit:0_conv:1</th>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>Where has she gone?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit:0_conv:1</th>\n",
       "      <td>sentimental</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>We no longer talk.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit:12424_conv:24848</th>\n",
       "      <td>sentimental</td>\n",
       "      <td>I found some pictures of my grandma in the att...</td>\n",
       "      <td>Yeah reminds me of the good old days.  I miss ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit:12424_conv:24849</th>\n",
       "      <td>surprised</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit:12424_conv:24849</th>\n",
       "      <td>surprised</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>Oh hey that's awesome!  That is awesome right?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit:12424_conv:24849</th>\n",
       "      <td>surprised</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>It is soooo awesome.  We have been wanting a b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hit:12424_conv:24849</th>\n",
       "      <td>surprised</td>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "      <td>That is awesome!!!! Congratulations!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76668 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     utterance_idx  \\\n",
       "hit:0_conv:1           sentimental   \n",
       "hit:0_conv:1           sentimental   \n",
       "hit:0_conv:1           sentimental   \n",
       "hit:0_conv:1           sentimental   \n",
       "hit:0_conv:1           sentimental   \n",
       "...                            ...   \n",
       "hit:12424_conv:24848   sentimental   \n",
       "hit:12424_conv:24849     surprised   \n",
       "hit:12424_conv:24849     surprised   \n",
       "hit:12424_conv:24849     surprised   \n",
       "hit:12424_conv:24849     surprised   \n",
       "\n",
       "                                                                context  \\\n",
       "hit:0_conv:1          I remember going to the fireworks with my best...   \n",
       "hit:0_conv:1          I remember going to the fireworks with my best...   \n",
       "hit:0_conv:1          I remember going to the fireworks with my best...   \n",
       "hit:0_conv:1          I remember going to the fireworks with my best...   \n",
       "hit:0_conv:1          I remember going to the fireworks with my best...   \n",
       "...                                                                 ...   \n",
       "hit:12424_conv:24848  I found some pictures of my grandma in the att...   \n",
       "hit:12424_conv:24849  I woke up this morning to my wife telling me s...   \n",
       "hit:12424_conv:24849  I woke up this morning to my wife telling me s...   \n",
       "hit:12424_conv:24849  I woke up this morning to my wife telling me s...   \n",
       "hit:12424_conv:24849  I woke up this morning to my wife telling me s...   \n",
       "\n",
       "                                                            speaker_idx  \n",
       "hit:0_conv:1          I remember going to see the fireworks with my ...  \n",
       "hit:0_conv:1          Was this a friend you were in love with_comma_...  \n",
       "hit:0_conv:1                        This was a best friend. I miss her.  \n",
       "hit:0_conv:1                                        Where has she gone?  \n",
       "hit:0_conv:1                                         We no longer talk.  \n",
       "...                                                                 ...  \n",
       "hit:12424_conv:24848  Yeah reminds me of the good old days.  I miss ...  \n",
       "hit:12424_conv:24849  I woke up this morning to my wife telling me s...  \n",
       "hit:12424_conv:24849     Oh hey that's awesome!  That is awesome right?  \n",
       "hit:12424_conv:24849  It is soooo awesome.  We have been wanting a b...  \n",
       "hit:12424_conv:24849               That is awesome!!!! Congratulations!  \n",
       "\n",
       "[76668 rows x 3 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hit:0_conv:1            sentimental I remember going to the fireworks ...\n",
       "hit:0_conv:1            sentimental I remember going to the fireworks ...\n",
       "hit:0_conv:1            sentimental I remember going to the fireworks ...\n",
       "hit:0_conv:1            sentimental I remember going to the fireworks ...\n",
       "hit:0_conv:1            sentimental I remember going to the fireworks ...\n",
       "                                              ...                        \n",
       "hit:12424_conv:24848    sentimental I found some pictures of my grandm...\n",
       "hit:12424_conv:24849    surprised I woke up this morning to my wife te...\n",
       "hit:12424_conv:24849    surprised I woke up this morning to my wife te...\n",
       "hit:12424_conv:24849    surprised I woke up this morning to my wife te...\n",
       "hit:12424_conv:24849    surprised I woke up this morning to my wife te...\n",
       "Length: 76668, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_text = df['utterance_idx'] + ' ' + df['context'] + ' ' + df['speaker_idx']\n",
    "training_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/damodargupta/Desktop/sentence-emotion analysis model- psyaa/training text.csv')\n",
    "df = df.drop('Unnamed: 0' , axis=1)\n",
    "df.to_csv('/Users/damodargupta/Desktop/sentence-emotion analysis model- psyaa/training text.csv' , index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76663</th>\n",
       "      <td>I found some pictures of my grandma in the att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76664</th>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76665</th>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76666</th>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76667</th>\n",
       "      <td>I woke up this morning to my wife telling me s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76668 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "0      I remember going to the fireworks with my best...\n",
       "1      I remember going to the fireworks with my best...\n",
       "2      I remember going to the fireworks with my best...\n",
       "3      I remember going to the fireworks with my best...\n",
       "4      I remember going to the fireworks with my best...\n",
       "...                                                  ...\n",
       "76663  I found some pictures of my grandma in the att...\n",
       "76664  I woke up this morning to my wife telling me s...\n",
       "76665  I woke up this morning to my wife telling me s...\n",
       "76666  I woke up this morning to my wife telling me s...\n",
       "76667  I woke up this morning to my wife telling me s...\n",
       "\n",
       "[76668 rows x 1 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['prompt']=df['prompt'].astype(str)\n",
    "df['prompt'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.batch_encode_plus(\n",
    "    training_text.tolist(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,  \n",
    "    return_tensors=\"pt\"\n",
    ")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'num_special_tokens_to_add'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[39m=\u001b[39m TextDataset(input_ids, file_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/Users/damodargupta/Desktop/sentence-emotion analysis model- psyaa/training text.csv\u001b[39;49m\u001b[39m'\u001b[39;49m , block_size\u001b[39m=\u001b[39;49m \u001b[39m64\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m data_collator \u001b[39m=\u001b[39m DataCollatorForLanguageModeling(tokenizer\u001b[39m=\u001b[39mtokenizer, mlm\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/my-env/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:63\u001b[0m, in \u001b[0;36mTextDataset.__init__\u001b[0;34m(self, tokenizer, file_path, block_size, overwrite_cache, cache_dir)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(file_path) \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput file path \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m not found\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m block_size \u001b[39m=\u001b[39m block_size \u001b[39m-\u001b[39m tokenizer\u001b[39m.\u001b[39;49mnum_special_tokens_to_add(pair\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     65\u001b[0m directory, filename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplit(file_path)\n\u001b[1;32m     66\u001b[0m cached_features_file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m     67\u001b[0m     cache_dir \u001b[39mif\u001b[39;00m cache_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m directory,\n\u001b[1;32m     68\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcached_lm_\u001b[39m\u001b[39m{\u001b[39;00mtokenizer\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mblock_size\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mfilename\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     69\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'num_special_tokens_to_add'"
     ]
    }
   ],
   "source": [
    "dataset = TextDataset(input_ids, file_path='/Users/damodargupta/Desktop/sentence-emotion analysis model- psyaa/training text.csv' , block_size= 64)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/Users/damodargupta/Desktop/sentence-emotion analysis model- psyaa/fine-tuning-blenderbot/fine_tuned_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,  \n",
    "    per_device_train_batch_size=4,  \n",
    "    save_steps=500, \n",
    "    save_total_limit=2, \n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=training_args.num_train_epochs, desc=\"Training\") as pbar:\n",
    "    def update_pbar_callback(eng):\n",
    "        pbar.update()\n",
    "    trainer.add_event_handler(Trainer.EPOCH_COMPLETED, update_pbar_callback)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"/Users/damodargupta/Desktop/sentence-emotion analysis model- psyaa/fine-tuning-blenderbot/fine_tuned_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
